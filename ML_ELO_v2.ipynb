{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning of Elo Merchant Category Recommendation Competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn import model_selection, preprocessing, metrics\n",
    "from sklearn.model_selection import StratifiedKFold,KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import Ridge, BayesianRidge\n",
    "import lightgbm as lgb\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../train.csv', parse_dates=[\"first_active_month\"])\n",
    "test_df = pd.read_csv('../test.csv', parse_dates=[\"first_active_month\"])\n",
    "\n",
    "print(\"Training data size\",train_df.shape)\n",
    "print(\"Testing data size\",test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df['elasped_time'] = (datetime.date(2018, 2, 1) - train_df['first_active_month'].dt.date).dt.days\n",
    "test_df['elasped_time'] = (datetime.date(2018, 2, 1) - test_df['first_active_month'].dt.date).dt.days\n",
    "\n",
    "train_df[\"year\"] = train_df[\"first_active_month\"].dt.year\n",
    "test_df[\"year\"] = test_df[\"first_active_month\"].dt.year\n",
    "train_df[\"month\"] = train_df[\"first_active_month\"].dt.month\n",
    "test_df[\"month\"] = test_df[\"first_active_month\"].dt.month\n",
    "\n",
    "train_df['outliers'] = 0\n",
    "train_df.loc[train_df['target']<-30,'outliers']=1\n",
    "train_df['outliers'].value_counts()\n",
    "\n",
    "target = train_df['target']\n",
    "del train_df['target']\n",
    "gc.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hist_df = pd.read_csv('../historical_transactions.csv')\n",
    "new_df = pd.read_csv(\"../new_merchant_transactions.csv\")\n",
    "\n",
    "print(\"Historical transactions data size\",hist_df.shape)\n",
    "print(\"New transactions data size\",new_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hd = {'Unique Entry': hist_df.nunique(axis = 0),\n",
    "        'Nan Entry': hist_df.isnull().any()}\n",
    "pd.DataFrame(data = hd, index = hist_df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for df in [hist_df,new_df]:\n",
    "#    df['category_2'].fillna(1.0,inplace=True)\n",
    "#    df['category_3'].fillna('A',inplace=True)\n",
    "#    df['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)\n",
    "for df in [hist_df,new_df]:\n",
    "    df['purchase_date'] = pd.to_datetime(df['purchase_date'])\n",
    "    df['year'] = df['purchase_date'].dt.year\n",
    "    df['weekofyear'] = df['purchase_date'].dt.weekofyear\n",
    "    df['month'] = df['purchase_date'].dt.month\n",
    "    df['dayofweek'] = df['purchase_date'].dt.dayofweek\n",
    "    df['weekend'] = (df.purchase_date.dt.weekday >=5).astype(int)\n",
    "    df['weekday'] = (df.purchase_date.dt.weekday <5).astype(int)\n",
    "    df['hour'] = df['purchase_date'].dt.hour\n",
    "    df['authorized_flag'] = df['authorized_flag'].map({'Y':1, 'N':0})\n",
    "    df['category_1'] = df['category_1'].map({'Y':1, 'N':0}) \n",
    "    #https://www.kaggle.com/c/elo-merchant-category-recommendation/discussion/73244\n",
    "    df['month_diff'] = ((datetime.datetime.today() - df['purchase_date']).dt.days)//30\n",
    "    df['month_diff'] += df['month_lag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hist_df = pd.get_dummies(hist_df,columns=['category_2','category_3'])\n",
    "new_df = pd.get_dummies(new_df,columns=['category_2','category_3'])\n",
    "\n",
    "agg_fun = {'authorized_flag': ['mean']}\n",
    "auth_mean = hist_df.groupby(['card_id']).agg(agg_fun)\n",
    "auth_mean.columns = ['_'.join(col).strip() for col in auth_mean.columns.values]\n",
    "auth_mean.reset_index(inplace=True)\n",
    "\n",
    "auth_df = hist_df[hist_df['authorized_flag'] == 1]\n",
    "hist_df = hist_df[hist_df['authorized_flag'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aggregate_historical_transactions(history):\n",
    "    \n",
    "    history.loc[:, 'purchase_date'] = pd.DatetimeIndex(history['purchase_date']).\\\n",
    "                                      astype(np.int64) * 1e-9\n",
    "    \n",
    "    agg_func = {\n",
    "        'category_1': ['sum', 'mean'],\n",
    "        'category_2_1.0': ['mean'],\n",
    "        'category_2_2.0': ['mean'],\n",
    "        'category_2_3.0': ['mean'],\n",
    "        'category_2_4.0': ['mean'],\n",
    "        'category_2_5.0': ['mean'],\n",
    "        'category_3_A': ['mean'],\n",
    "        'category_3_B': ['mean'],\n",
    "        'category_3_C': ['mean'],\n",
    "        'merchant_id': ['nunique'],\n",
    "        'merchant_category_id':['nunique'],\n",
    "        'state_id':['nunique'],\n",
    "        'city_id': ['nunique'],\n",
    "        'subsector_id':['nunique'],\n",
    "        'year':['nunique'],\n",
    "        'month':['nunique'],\n",
    "        'weekofyear':['nunique'],        \n",
    "        'weekend':['sum','mean'],\n",
    "        'weekday':['sum','mean'],\n",
    "        # non-categorical features\n",
    "        'purchase_amount': ['sum', 'median', 'max', 'min', 'std'],\n",
    "        'installments': ['sum', 'median', 'max', 'min', 'std'],\n",
    "        'purchase_date': [np.ptp],\n",
    "        'month_lag': ['min', 'max','mean','std'],\n",
    "        'month_diff':['mean'],\n",
    "        'authorized_flag': ['sum', 'mean'],\n",
    "        }\n",
    "    agg_history = history.groupby(['card_id']).agg(agg_func)\n",
    "    agg_history.columns = ['_'.join(col).strip() \n",
    "                           for col in agg_history.columns.values]\n",
    "    agg_history.reset_index(inplace=True)\n",
    "    \n",
    "    df = (history.groupby('card_id')\n",
    "          .size()\n",
    "          .reset_index(name='hist_transactions_count'))\n",
    "    \n",
    "    agg_history = pd.merge(df, agg_history, on='card_id', how='left')\n",
    "    \n",
    "    return agg_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "history = aggregate_historical_transactions(hist_df)\n",
    "history.columns = ['hist_' + c if c != 'card_id' else c for c in history.columns]\n",
    "history[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "authorized = aggregate_historical_transactions(auth_df)\n",
    "authorized.columns = ['auth_' + c if c != 'card_id' else c for c in authorized.columns]\n",
    "authorized[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new = aggregate_historical_transactions(new_df)\n",
    "new.columns = ['new_' + c if c != 'card_id' else c for c in new.columns]\n",
    "new[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def aggregate_per_month(history):\n",
    "    grouped = history.groupby(['card_id', 'month_lag'])\n",
    "\n",
    "    agg_func = {\n",
    "            'purchase_amount': ['count', 'sum', 'mean', 'min', 'max', 'std'],\n",
    "            'installments': ['count', 'sum', 'mean', 'min', 'max', 'std'],\n",
    "            }\n",
    "\n",
    "    intermediate_group = grouped.agg(agg_func)\n",
    "    intermediate_group.columns = ['_'.join(col).strip() for col in intermediate_group.columns.values]\n",
    "    intermediate_group.reset_index(inplace=True)\n",
    "\n",
    "    final_group = intermediate_group.groupby('card_id').agg(['mean', 'std'])\n",
    "    final_group.columns = ['_'.join(col).strip() for col in final_group.columns.values]\n",
    "    final_group.reset_index(inplace=True)\n",
    "    \n",
    "    return final_group\n",
    "#___________________________________________________________\n",
    "final_group =  aggregate_per_month(auth_df) \n",
    "final_group[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def successive_aggregates(df, field1, field2):\n",
    "    t = df.groupby(['card_id', field1])[field2].mean()\n",
    "    u = pd.DataFrame(t).reset_index().groupby('card_id')[field2].agg(['mean', 'min', 'max', 'std'])\n",
    "    u.columns = [field1 + '_' + field2 + '_' + col for col in u.columns.values]\n",
    "    u.reset_index(inplace=True)\n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "additional_fields = successive_aggregates(new_df, 'category_1', 'purchase_amount')\n",
    "additional_fields = additional_fields.merge(successive_aggregates(new_df, 'installments', 'purchase_amount'),\n",
    "                                            on = 'card_id', how='left')\n",
    "additional_fields = additional_fields.merge(successive_aggregates(new_df, 'city_id', 'purchase_amount'),\n",
    "                                            on = 'card_id', how='left')\n",
    "additional_fields = additional_fields.merge(successive_aggregates(new_df, 'category_1', 'installments'),\n",
    "                                            on = 'card_id', how='left')\n",
    "del new_df\n",
    "del auth_df\n",
    "del hist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.merge(train_df, history, on='card_id', how='left')\n",
    "test_df = pd.merge(test_df, history, on='card_id', how='left')\n",
    "\n",
    "train_df = pd.merge(train_df, authorized, on='card_id', how='left')\n",
    "test_df = pd.merge(test_df, authorized, on='card_id', how='left')\n",
    "\n",
    "train_df = pd.merge(train_df, new, on='card_id', how='left')\n",
    "test_df = pd.merge(test_df, new, on='card_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.merge(train_df, final_group, on='card_id', how='left')\n",
    "test_df = pd.merge(test_df, final_group, on='card_id', how='left')\n",
    "\n",
    "train_df = pd.merge(train_df, additional_fields, on='card_id', how='left')\n",
    "test_df = pd.merge(test_df, additional_fields, on='card_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.merge(train_df, auth_mean, on='card_id', how='left')\n",
    "test_df = pd.merge(test_df, auth_mean, on='card_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train_df['outliers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_df.to_csv('../test_v5_stack.csv')\n",
    "train_df.to_csv('../train_v5_stack.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = [c for c in train_df.columns if c not in ['card_id', 'first_active_month']]\n",
    "#features = [f for f in features if f not in unimportant_features]\n",
    "categorical_feats = ['feature_1','feature_2', 'feature_3','year','month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param = {'num_leaves': 31,\n",
    "         'min_data_in_leaf': 32, \n",
    "         'objective':'regression',\n",
    "         'max_depth': -1,\n",
    "         'learning_rate': 0.005,\n",
    "         \"min_child_samples\": 20,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.9,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.9 ,\n",
    "         \"bagging_seed\": 11,\n",
    "         \"metric\": 'rmse',\n",
    "         \"lambda_l1\": 0.1,\n",
    "         \"nthread\": 4,\n",
    "         \"verbosity\": -1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/fabiendaniel/hyperparameter-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param = {'num_leaves': 111,\n",
    "         'min_data_in_leaf': 149, \n",
    "         'objective':'regression',\n",
    "         'max_depth': 9,\n",
    "         'learning_rate': 0.005,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.7522,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.7083 ,\n",
    "         \"bagging_seed\": 11,\n",
    "         \"metric\": 'rmse',\n",
    "         \"lambda_l1\": 0.2634,\n",
    "         \"random_state\": 133,\n",
    "         \"verbosity\": -1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=4590)\n",
    "#from sklearn.model_selection import StratifiedKFold,KFold,RepeatedKFold\n",
    "#from sklearn.model_selection import RepeatedKFold\n",
    "#folds = RepeatedKFold(n_splits=5, n_repeats=2, random_state=4520)\n",
    "oof = np.zeros(len(train_df))\n",
    "predictions = np.zeros(len(test_df))\n",
    "start = time.time()\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_df.values, target.values)):\n",
    "    print(\"fold nÂ°{}\".format(fold_))\n",
    "    trn_data = lgb.Dataset(train_df.iloc[trn_idx][features],\n",
    "                           label=target.iloc[trn_idx],\n",
    "                           categorical_feature=categorical_feats\n",
    "                          )\n",
    "    val_data = lgb.Dataset(train_df.iloc[val_idx][features],\n",
    "                           label=target.iloc[val_idx],\n",
    "                           categorical_feature=categorical_feats\n",
    "                          )\n",
    "\n",
    "    num_round = 10000\n",
    "    clf = lgb.train(param,\n",
    "                    trn_data,\n",
    "                    num_round,\n",
    "                    valid_sets = [trn_data, val_data],\n",
    "                    verbose_eval=100,\n",
    "                    early_stopping_rounds = 200)\n",
    "    \n",
    "    oof[val_idx] = clf.predict(train_df.iloc[val_idx][features], num_iteration=clf.best_iteration)\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = features\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    predictions += clf.predict(test_df[features], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.5f}\".format(mean_squared_error(oof, target)**0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = (feature_importance_df[[\"feature\", \"importance\"]]\n",
    "        .groupby(\"feature\")\n",
    "        .mean()\n",
    "        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n",
    "\n",
    "best_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n",
    "\n",
    "plt.figure(figsize=(14,25))\n",
    "sns.barplot(x=\"importance\",\n",
    "            y=\"feature\",\n",
    "            data=best_features.sort_values(by=\"importance\",\n",
    "                                           ascending=False))\n",
    "plt.title('LightGBM Features (avg over folds)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('lgbm_importances_v5.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub_df = pd.DataFrame({\"card_id\":test_df[\"card_id\"].values})\n",
    "sub_df[\"target\"] = predictions\n",
    "sub_df.to_csv(\"../submit_v4_strat.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
